# yaml-language-server: $schema="https://raw.githubusercontent.com/ansible-community/schemas/main/f/ansible.json#/$defs/tasks"
# $schema: https://raw.githubusercontent.com/ansible-community/schemas/main/f/ansible.json#/$defs/tasks
---
# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @see {@link https://docs.ansible.com/ansible/2.3/include_vars_module.html}
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: Reset all previous Kubernetes Configurations.
  include: "{{ inventory_dir }}/tasks/kubeadm-reset.yml"
  when:
    - "ansible_facts['distribution'] == 'CentOS'"
    - "'master' in inventory_hostname or 'worker' in inventory_hostname"
  delegate_to: masters, workers

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @description: This command will delete all files within specified Directory.
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: Create Kubernetes PKI Directories.
  file:
    path: /etc/kubernetes/pki
    state: directory
    mode: 0755

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @description: [Ansible] - Add Kubernetes Initialize Configuration within Google Cloud Compute Engine.
# @see {@link https://www.middlewareinventory.com/blog/ansible-get-ip-address/}.
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: Edit `kubeadm.conf` for [Master && Worker] Nodes.
  vars:
    CLOUD_PROVIDER: "external"
    NODE_IP: "{{ ansible_eth0.ipv4.address }}"
  template:
    src: "{{ inventory_dir }}/templates/kubeadm.conf"
    dest: /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf

- name: Force `systemd` reloading all configurations `(Ansible v2.4+)`
  systemd:
    daemon_reload: yes

- name: 'Restart service `kubelet`, in all cases.'
  service:
    name: kubelet
    state: restarted
    daemon-reload: yes

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @description: [Ansible] - Add Kubernetes Initialize Configuration within Google Cloud Compute Engine.
# @see {@link https://docs.openshift.com/container-platform/3.11/install_config/configuring_gce.html}
# @see {@link https://medium.com/codex/kubernetes-cluster-on-gce-beyond-kubeadm-4a954935e2c8/}
# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @description: [Ansible] - Provide Kubernetes Cloud Configuration within Google Cloud Compute Engine.
# @see {@link https://stackoverflow.com/questions/67600909/how-to-add-external-gcp-loadbalancer-to-kubespray-cluster/}
# @see {@link https://github.com/kubernetes/ingress-nginx/issues/462#issuecomment-332334787}
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: Add Kubernetes Cloud Configuration into Initial Master Nodes.
  template:
    src: "{{ inventory_dir }}/templates/google-compute-engine.conf"
    dest: /etc/kubernetes/google-compute-engine.conf
    force: yes # Override when file exists. Actually `kubeadm-reset.yml` already deleted the folder `/etc/kubernetes` before bootstrap happened.
  vars:
    GCP_PROJECT_ID: "{{ gcp_master_04_project_id }}"
    NODE_HOSTNAME: "{{ inventory_hostname }}"
    NODE_PREFIX: "master"
    NODE_TAGS: "masters"
  when:
    - "ansible_facts['distribution'] == 'CentOS'"
    - "'master' in ansible_hostname or 'master' in inventory_hostname"

- name: Add Kubernetes Cloud Configuration into Initial Worker Group Nodes::[01-04].
  template:
    src: "{{ inventory_dir }}/templates/google-compute-engine.conf"
    dest: /etc/kubernetes/google-compute-engine.conf
    force: yes # Override when file exists. Actually `kubeadm-reset.yml` already deleted the folder `/etc/kubernetes` before bootstrap happened.
  vars:
    GCP_PROJECT_ID: "{{ gcp_worker_04_project_id }}"
    NODE_HOSTNAME: "{{ inventory_hostname }}"
    NODE_PREFIX: "worker"
    NODE_TAGS: "workers"
  when:
    - "ansible_facts['distribution'] == 'CentOS'"
    - "'worker' in ansible_hostname or 'worker' in inventory_hostname"
    - "'worker-01' <= inventory_hostname and inventory_hostname <= 'worker-04'"

- name: Add Kubernetes Cloud Configuration into Initial Worker Group Nodes::[05-08].
  template:
    src: "{{ inventory_dir }}/templates/google-compute-engine.conf"
    dest: /etc/kubernetes/google-compute-engine.conf
    force: yes # Override when file exists. Actually `kubeadm-reset.yml` already deleted the folder `/etc/kubernetes` before bootstrap happened.
  vars:
    GCP_PROJECT_ID: "{{ gcp_worker_08_project_id }}"
    NODE_HOSTNAME: "{{ inventory_hostname }}"
    NODE_PREFIX: "worker"
    NODE_TAGS: "workers"
  when:
    - "ansible_facts['distribution'] == 'CentOS'"
    - "'worker' in ansible_hostname or 'worker' in inventory_hostname"
    - "'worker-05' <= inventory_hostname and inventory_hostname <= 'worker-08'"

- name: Add Kubernetes Cloud Configuration into Initial Worker Group Nodes::[09-12].
  template:
    src: "{{ inventory_dir }}/templates/google-compute-engine.conf"
    dest: /etc/kubernetes/google-compute-engine.conf
    force: yes # Override when file exists. Actually `kubeadm-reset.yml` already deleted the folder `/etc/kubernetes` before bootstrap happened.
  vars:
    GCP_PROJECT_ID: "{{ gcp_worker_12_project_id }}"
    NODE_HOSTNAME: "{{ inventory_hostname }}"
    NODE_PREFIX: "worker"
    NODE_TAGS: "workers"
  when:
    - "ansible_facts['distribution'] == 'CentOS'"
    - "'worker' in ansible_hostname or 'worker' in inventory_hostname"
    - "'worker-09' <= inventory_hostname and inventory_hostname <= 'worker-12'"

- name: Add Kubernetes Cloud Configuration into Initial Worker Group Nodes::[13-24].
  template:
    src: "{{ inventory_dir }}/templates/google-compute-engine.conf"
    dest: /etc/kubernetes/google-compute-engine.conf
    force: yes # Override when file exists. Actually `kubeadm-reset.yml` already deleted the folder `/etc/kubernetes` before bootstrap happened.
  vars:
    GCP_PROJECT_ID: "{{ gcp_project_id }}"
    NODE_HOSTNAME: "{{ inventory_hostname }}"
    NODE_PREFIX: "worker"
    NODE_TAGS: "workers"
  when:
    - "ansible_facts['distribution'] == 'CentOS'"
    - "'worker' in ansible_hostname or 'worker' in inventory_hostname"
    - "'worker-13' <= inventory_hostname and inventory_hostname <= 'worker-24'"

- name: Add Kubernetes Admin Configuration into Initial Master Node.
  template:
    src: "{{ inventory_dir }}/templates/kubeadm-config.yaml"
    dest: /etc/kubernetes/pki/kubeadm-config.yaml
  vars:
    CONTROL_PLANE_ENDPOINT: "{{ control_plane_endpoint }}"
  delegate_to: master-01
  run_once: yes

- name: Initialize Kubernetes Cluster within the first Master Node.
  shell: kubeadm init --config /etc/kubernetes/pki/kubeadm-config.yaml;
  delegate_to: master-01
  run_once: yes

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @description: [Ansible] - By Default `kubectl` looks for file named `config` in the `$HOME/.kube` directory.
# @see {@link https://kubernetes.io/docs/reference/kubectl/}
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: Create Directory for `Kube-Config` File.
  file:
    path: ~/.kube
    state: directory
    owner: root
    mode: 0755
  when: "'master' in inventory_hostname"

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @description: [Ansible] - By Default `kubectl` looks for file named `config` in the `$HOME/.kube` directory. Otherwise `kubectl` from Ansible failed.
# @see {@link https://kubernetes.io/docs/reference/kubectl/}
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: Copy `/etc/kubernetes/admin.conf` to SSH-User Home Directory.
  copy:
    remote_src: yes
    src: /etc/kubernetes/admin.conf
    dest: ~/.kube/config
    owner: root
    mode: "u=rw,g=r,o=r" # Mode::[[ {read: 4}, {write: 2}, {execute: 1} ]]
  delegate_to: master-01
  run_once: yes

- name: Create Symbolic Link from `/etc/kubernetes/admin.conf` to SSH-User Home Directory.
  file:
    src: /etc/kubernetes/admin.conf
    dest: ~/.kube/config
    state: link
    owner: root
    mode: "u=rw,g=r,o=r" # Mode::[[ {read: 4}, {write: 2}, {execute: 1} ]]
    force: True # Force Symlink override exist files.
  delegate_to: master-01
  run_once: yes
  when: False # Currently Disable. Somehow symlink has full permission [0777].

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @description: [Ansible] - Kubernetes [Control-Plane] Load-Balancer sometimes direct into wrong Master Node. Causing failed for apply command.
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: Deploy Flannel CNI Network.
  shell: kubectl apply --filename https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml;
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf # Allow `kubectl` to read Kubernetes Configuration file.
  retries: 10 # Max Retry Number until Failed.
  delay: 3 # Delay Seconds for each Retry.
  delegate_to: master-01
  run_once: yes

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @description: [Ansible] - Get discovery Certificate Authority (CA) Token Hash.
# @see {@link https://github.com/kubernetes/kubeadm/issues/659#issuecomment-396070894}
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: Get Kubernetes Token and Token-Hash.
  block:
    - name: Register Kubernetes join Command.
      shell: kubeadm token create --print-join-command
      register: kubernetes_join_command
      delegate_to: master-01
      run_once: yes

    - name: Filter Kubernetes Token using RegExp.
      set_fact:
        token: "{{ kubernetes_join_command.stdout | regex_search(regexp, '\\2') | first }}"
      vars:
        regexp: '([^\s]+\s){4}([^\s]+)'
      delegate_to: master-01
      run_once: yes

    - name: Filter Kubernetes Token Hash using RegExp.
      set_fact:
        token_hash: "{{ kubernetes_join_command.stdout | regex_search(regexp, '\\2') | first }}"
      vars:
        regexp: '([^\s]+\s){6}([^\s]+)'
      delegate_to: master-01
      run_once: yes

    - name: Debug Logging all Registered Variables.
      debug:
        msg: "{{ item.key }}: {{ item.value }}"
      delegate_to: master-01
      run_once: yes
      loop:
        - { key: token, value: "{{ token }}" }
        - { key: token_hash, value: "{{ token_hash }}" }

  delegate_to: master-01
  run_once: yes

- name: Generate Kubernetes Certificate Key.
  shell: kubeadm init phase upload-certs --upload-certs
  register: kubernetes_certificate_key
  delegate_to: master-01
  run_once: yes

- name: Pause 2 seconds to make sure Certificate got uploaded.
  pause:
    seconds: 2

- name: Print Join Command for Master Nodes within Kubernetes Cluster.
  debug:
    msg: "kubeadm join {{ control_plane_endpoint }} --control-plane --token {{ token }} --discovery-token-ca-cert-hash {{ token_hash }} --certificate-key {{ kubernetes_certificate_key.stdout_lines[2] }}"
  delegate_to: master-01
  run_once: yes

- name: Bootstrap Master Nodes within Kubernetes Cluster.
  block:
    - name: Join Master Nodes within Kubernetes Cluster.
      shell: >
        kubeadm join {{ control_plane_endpoint }}
        --control-plane
        --token {{ token }}
        --discovery-token-ca-cert-hash {{ token_hash }}
        --certificate-key {{ kubernetes_certificate_key.stdout_lines[2] }}

    - name: Copy `/etc/kubernetes/admin.conf` to SSH-User Home Directory.
      copy:
        remote_src: yes
        src: /etc/kubernetes/admin.conf
        dest: ~/.kube/config
        owner: root
        mode: "u=rw,g=r,o=r" # Mode::[[ {read: 4}, {write: 2}, {execute: 1} ]]

  when: "'master' in inventory_hostname and inventory_hostname != 'master-01'"

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @deprecated: [Add / Join] Workers to Kubernetes Cluster Nodes Simultaneously (Parallelism).
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: Bootstrap Worker Nodes within Kubernetes Cluster.
  block:
    - name: Join Worker Nodes within Kubernetes Cluster.
      shell: >
        kubeadm join {{ control_plane_endpoint }}
        --token {{ token }}
        --discovery-token-ca-cert-hash {{ token_hash }}

    - name: Restart kubelet service
      service:
        name: kubelet
        daemon-reload: yes
        state: restarted

  when: "'worker' in inventory_hostname"

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @deprecated: [Add / Join] Workers to Kubernetes Cluster Nodes Sequentially (Concurrency).
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: Join Worker Nodes within Kubernetes Cluster.
  shell: >
    kubeadm join {{ control_plane_endpoint }}
    --token {{ token }}
    --discovery-token-ca-cert-hash {{ token_hash }}
  run_once: yes
  delegate_to: "{{ item }}"
  loop: "{{ groups.workers }}"
  when: False # Currently Disable.

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @description: [Add / Remove] Roles to Cluster Nodes.
# @see {@link https://stackoverflow.com/questions/48854905/how-to-add-roles-to-nodes-in-kubernetes}.
# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @description: [Ansible] - Ansible Tasks with [Retry] on every Iteration until [SUCCESS].
# @see {@link https://github.com/ansible/ansible/issues/44741#issuecomment-416632200}.
# @see {@link https://stackoverflow.com/questions/70668326/ansible-async-status-task-error-ansible-job-id-undefined-variable}.
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: Set Role Labels for [Master / Control-Plane] Nodes.
  shell: >
    kubectl label --overwrite node {{ item }}.e8s.io node-role.kubernetes.io/master=""
  loop: "{{ groups.masters }}"
  loop_control:
    pause: 0 # Sleep Timeout between each Iteration.
  delegate_to: master-01
  ignore_errors: True
  run_once: yes
  register: micro_tasks
  until: micro_tasks is succeeded
  retries: 3 # Each Iteration will [RETRY_MAX=3] times.

- name: Set Role Labels for [Worker] Nodes.
  shell: >
    kubectl label --overwrite node {{ item }}.e8s.io node-role.kubernetes.io/worker=""
  loop: "{{ groups.workers }}"
  loop_control:
    pause: 0 # Sleep Timeout between each Iteration.
  delegate_to: master-01
  ignore_errors: True
  run_once: yes
  register: micro_tasks
  until: micro_tasks is succeeded
  retries: 3 # Each Iteration will [RETRY_MAX=3] times.

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @description: [Remove] Taints from [Masters / Workers] Cluster Nodes.
# @description: Specifying [--cloud-provider=external] flag on Kubelet will mark [`${node.cloudprovider.kubernetes.io/uninitialized=NoSchedule}`].
# @see {@link https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/}.
# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @description: [Ansible] - Ansible Tasks with [Retry] on every Iteration until [SUCCESS].
# @see {@link https://github.com/ansible/ansible/issues/44741#issuecomment-416632200}.
# @see {@link https://stackoverflow.com/questions/70668326/ansible-async-status-task-error-ansible-job-id-undefined-variable}.
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: Remove Taints [Uninitialized::NoSchedule] for [Master / Control-Plane] Nodes.
  shell: >
    kubectl taint node {{ item }}.e8s.io node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule-
  loop: "{{ groups.masters }}"
  loop_control:
    pause: 0 # Sleep Timeout between each Iteration.
  delegate_to: master-01
  ignore_errors: True
  run_once: yes
  register: micro_tasks
  until: micro_tasks is succeeded
  retries: 3 # Each Iteration will [RETRY_MAX=3] times.

- name: Remove Taints [Uninitialized::NoSchedule] for [Worker] Nodes.
  shell: >
    kubectl taint node {{ item }}.e8s.io node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule-
  loop: "{{ groups.workers }}"
  loop_control:
    pause: 0 # Sleep Timeout between each Iteration.
  delegate_to: master-01
  ignore_errors: True
  run_once: yes
  register: micro_tasks
  until: micro_tasks is succeeded
  retries: 3 # Each Iteration will [RETRY_MAX=3] times.

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @see {@link https://serverfault.com/questions/691080/how-to-fetch-multiple-files-from-remote-machine-to-local-with-ansible/}
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: List all Files need to copy from Remote Machine to Local Ansible Machine.
  shell: find ~/.kube/* -maxdepth 1 -type f | cut --delimiter '~' --fields=2
  register: context_kubernetes
  delegate_to: master-01
  run_once: yes
  when: False # Currently Disable.

- name: "[[Fetch && Pull]] all Files from Remote Machine to Local Ansible Machine."
  fetch:
    src: "{{ item }}"
    dest: ~/.kube/
  with_items: "{{ context_kubernetes.stdout_lines }}"
  delegate_to: master-01
  run_once: yes
  when: False # Currently Disable.

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @see {@link https://stackoverflow.com/questions/54152051/ansible-fetch-a-collection-of-files-to-the-destination-without-copying-remote/}
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: List all Files need to copy from Remote Machine to Local Ansible Machine.
  find:
    paths: "~/.kube"
    recurse: false
  register: context_kubernetes
  delegate_to: master-01
  run_once: yes
  when: False # Currently Disable.

- name: "[[Fetch && Pull]] all Files from Remote Machine to Local Ansible [Admin] Machine."
  block:
    - fetch:
        src: "{{ item.path }}"
        dest: ~/.kube/
        flat: yes
      with_items: "{{ context_kubernetes.files }}"
      delegate_to: master-01
      run_once: yes

    - fetch:
        src: "{{ item.path }}"
        dest: ~/.kube/
        flat: yes
      with_items: "{{ context_kubernetes.files }}"
      delegate_to: master-01
      run_once: yes
  when: False # Currently Disable.

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @see {@link https://stackoverflow.com/questions/56048959/ansible-local-action-example-how-does-it-work/}
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: Pull Kubernetes Admin Context from Remote Master Nodes.
  fetch:
    src: /etc/kubernetes/admin.conf
    dest: /etc/kubernetes/admin.conf
    flat: yes
  delegate_to: master-01
  run_once: yes

- name: Copy Kubernetes Admin Context into Specific User Directory.
  local_action:
    module: copy
    src: /etc/kubernetes/admin.conf
    dest: ~/.kube/config

- name: Suspend 2 seconds before deploying Kubernetes Metrics Server.
  pause:
    seconds: 2

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @description: [Add / Deploy] Metrics Server to Kubernetes Cluster.
# @see {@link https://computingforgeeks.com/how-to-deploy-metrics-server-to-kubernetes-cluster/}
# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @description: [Ansible] - Run Ansible Tasks locally without delegate_to `localhost`.
# @see {@link https://stackoverflow.com/questions/18900236/run-command-on-the-ansible-host#43220691}
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: Deploy Metrics Server to Kubernetes Cluster within Admin Context.
  local_action:
    module: shell
    _raw_params: kubectl apply --filename {{ inventory_dir }}/resources/kubernetes/metrics-server-v0.6.1.yaml
  run_once: yes

- name: Set Default [Kube-Context] and [Kube-Environment].
  connection: local
  delegate_to: localhost
  shell: kubens kube-system
  run_once: yes

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @description: [Ansible] - `systemctl daemon-reload` must be executed so that Workers can be visible to Masters.
# @see {@link https://github.com/kubernetes/kubernetes/issues/61224#issuecomment-397404671}
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: Restart Service Containerd on CentOS. In all cases, also issue daemon-reload to pick-up Config changes.
  systemd:
    name: containerd
    state: restarted
    daemon_reload: yes

# ----------------------------------------------------------------------------------------------------------------------------------------------------
# @description: [Ansible] - Add Kubernetes Initialize Configuration within Google Cloud Compute Engine.
# @see {@link https://www.middlewareinventory.com/blog/ansible-get-ip-address/}.
# ----------------------------------------------------------------------------------------------------------------------------------------------------
- name: Edit `kubeadm.conf` for [Master && Worker] Nodes.
  vars:
    CLOUD_PROVIDER: "gce"
    NODE_IP: "{{ ansible_eth0.ipv4.address }}"
  template:
    src: "{{ inventory_dir }}/templates/kubeadm.conf"
    dest: /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf

- name: Force `systemd` reloading all configurations `(Ansible v2.4+)`
  systemd:
    daemon_reload: yes

- name: 'Restart service `kubelet`, in all cases.'
  service:
    name: kubelet
    state: restarted
    daemon-reload: yes
